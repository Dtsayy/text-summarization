{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"attention+model summary.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNx7kPaW3FL2c9PD6TzRcmT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ptQmTlaEZkIl"},"outputs":[],"source":["class AttentionLayer(Layer):\n","    \"\"\"\n","    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n","    There are three sets of weights introduced W_a, U_a, and V_a\n","     \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert isinstance(input_shape, list)\n","        # Create a trainable weight variable for this layer.\n","\n","        self.W_a = self.add_weight(name='W_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.U_a = self.add_weight(name='U_a',\n","                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.V_a = self.add_weight(name='V_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","\n","        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, inputs, verbose=False):\n","        \"\"\"\n","        inputs: [encoder_output_sequence, decoder_output_sequence]\n","        \"\"\"\n","        assert type(inputs) == list\n","        encoder_out_seq, decoder_out_seq = inputs\n","        if verbose:\n","            print('encoder_out_seq>', encoder_out_seq.shape)\n","            print('decoder_out_seq>', decoder_out_seq.shape)\n","\n","        def energy_step(inputs, states):\n","            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n","\n","            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n","            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n","\n","            \"\"\" Some parameters required for shaping tensors\"\"\"\n","            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n","            de_hidden = inputs.shape[-1]\n","\n","            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n","            # <= batch_size*en_seq_len, latent_dim\n","            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n","            # <= batch_size*en_seq_len, latent_dim\n","            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n","            if verbose:\n","                print('wa.s>',W_a_dot_s.shape)\n","\n","            \"\"\" Computing hj.Ua \"\"\"\n","            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n","            if verbose:\n","                print('Ua.h>',U_a_dot_h.shape)\n","\n","            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n","            # <= batch_size*en_seq_len, latent_dim\n","            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n","            if verbose:\n","                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n","\n","            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n","            # <= batch_size, en_seq_len\n","            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n","            # <= batch_size, en_seq_len\n","            e_i = K.softmax(e_i)\n","\n","            if verbose:\n","                print('ei>', e_i.shape)\n","\n","            return e_i, [e_i]\n","\n","        def context_step(inputs, states):\n","            \"\"\" Step function for computing ci using ei \"\"\"\n","            # <= batch_size, hidden_size\n","            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n","            if verbose:\n","                print('ci>', c_i.shape)\n","            return c_i, [c_i]\n","\n","        def create_inital_state(inputs, hidden_size):\n","            # We are not using initial states, but need to pass something to K.rnn funciton\n","            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n","            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n","            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n","            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n","            return fake_state\n","\n","        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n","        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n","\n","        \"\"\" Computing energy outputs \"\"\"\n","        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n","        last_out, e_outputs, _ = K.rnn(\n","            energy_step, decoder_out_seq, [fake_state_e],\n","        )\n","\n","        \"\"\" Computing context vectors \"\"\"\n","        last_out, c_outputs, _ = K.rnn(\n","            context_step, e_outputs, [fake_state_c],\n","        )\n","\n","        return c_outputs, e_outputs\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\" Outputs produced by the layer \"\"\"\n","        return [\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n","        ]"]},{"cell_type":"code","source":[""],"metadata":{"id":"kSIBLYLRZt0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["K.clear_session()\n","\n","embedding_dim = 150 #Size of word embeddings.\n","latent_dim = 250 #No. of neurons in LSTM layer.\n","\n","encoder_input = Input(shape=(1200, ))\n","encoder_emb = Embedding(news_vocab, embedding_dim, trainable=True)(encoder_input) #Embedding Layer\n","\n","#Three-stacked LSTM layers for encoder. Return_state returns the activation state vectors, a(t) and c(t), return_sequences return the output of the neurons y(t).\n","#With layers stacked one above the other, y(t) of previous layer becomes x(t) of next layer.\n","encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n","y_1, a_1, c_1 = encoder_lstm1(encoder_emb)\n","\n","encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n","y_2, a_2, c_2 = encoder_lstm2(y_1)\n","\n","encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n","encoder_output, a_enc, c_enc = encoder_lstm3(y_2)\n","\n","#Single LSTM layer for decoder followed by Dense softmax layer to predict the next word in summary.\n","decoder_input = Input(shape=(250,))\n","decoder_emb = Embedding(headline_vocab, embedding_dim, trainable=True)(decoder_input)\n","\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n","decoder_output, decoder_fwd, decoder_back = decoder_lstm(decoder_emb, initial_state=[a_enc, c_enc]) #Final output states of encoder last layer are fed into decoder.\n","\n","#Attention Layer\n","attn_layer = AttentionLayer(name='attention_layer') \n","attn_out, attn_states = attn_layer([encoder_output, decoder_output]) \n","\n","decoder_concat_input = tf.keras.layers.Concatenate(axis=-1, name='concat_layer')([decoder_output, attn_out])\n","\n","decoder_dense = TimeDistributed(Dense(headline_vocab, activation='softmax'))\n","decoder_output = decoder_dense(decoder_concat_input)\n","\n","model = Model([encoder_input, decoder_input], decoder_output)\n","model.summary()"],"metadata":{"id":"3cv6Rx0bZt3m"},"execution_count":null,"outputs":[]}]}